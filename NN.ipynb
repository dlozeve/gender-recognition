{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn\")\n",
    "plt.rcParams[\"figure.figsize\"] = 10, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, brier_score_loss, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"data/train.data.csv\")\n",
    "y = pd.read_csv(\"data/train.labels.csv\")\n",
    "test_data = pd.read_csv(\"data/test.data.csv\")\n",
    "X = preprocessing.scale(X.values)\n",
    "y = y.values.ravel()\n",
    "test_data = preprocessing.scale(test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=200,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = torch.utils.data.TensorDataset(torch.Tensor(X_test), torch.LongTensor(y_test))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=200,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 500)\n",
    "        self.fc2 = nn.Linear(500, 200)\n",
    "        #self.fc3 = nn.Linear(500, 100)\n",
    "        self.fc4 = nn.Linear(500, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.056\n",
      "[1,    20] loss: 0.033\n",
      "[1,    30] loss: 0.025\n",
      "[1,    40] loss: 0.020\n",
      "[1,    50] loss: 0.015\n",
      "[1,    60] loss: 0.014\n",
      "[2,    10] loss: 0.011\n",
      "[2,    20] loss: 0.009\n",
      "[2,    30] loss: 0.010\n",
      "[2,    40] loss: 0.008\n",
      "[2,    50] loss: 0.008\n",
      "[2,    60] loss: 0.008\n",
      "[3,    10] loss: 0.006\n",
      "[3,    20] loss: 0.005\n",
      "[3,    30] loss: 0.006\n",
      "[3,    40] loss: 0.006\n",
      "[3,    50] loss: 0.006\n",
      "[3,    60] loss: 0.006\n",
      "[4,    10] loss: 0.004\n",
      "[4,    20] loss: 0.003\n",
      "[4,    30] loss: 0.004\n",
      "[4,    40] loss: 0.004\n",
      "[4,    50] loss: 0.004\n",
      "[4,    60] loss: 0.004\n",
      "[5,    10] loss: 0.003\n",
      "[5,    20] loss: 0.003\n",
      "[5,    30] loss: 0.003\n",
      "[5,    40] loss: 0.003\n",
      "[5,    50] loss: 0.003\n",
      "[5,    60] loss: 0.003\n",
      "[6,    10] loss: 0.002\n",
      "[6,    20] loss: 0.002\n",
      "[6,    30] loss: 0.002\n",
      "[6,    40] loss: 0.002\n",
      "[6,    50] loss: 0.002\n",
      "[6,    60] loss: 0.002\n",
      "[7,    10] loss: 0.001\n",
      "[7,    20] loss: 0.002\n",
      "[7,    30] loss: 0.002\n",
      "[7,    40] loss: 0.001\n",
      "[7,    50] loss: 0.001\n",
      "[7,    60] loss: 0.001\n",
      "[8,    10] loss: 0.001\n",
      "[8,    20] loss: 0.001\n",
      "[8,    30] loss: 0.001\n",
      "[8,    40] loss: 0.001\n",
      "[8,    50] loss: 0.001\n",
      "[8,    60] loss: 0.001\n",
      "[9,    10] loss: 0.001\n",
      "[9,    20] loss: 0.001\n",
      "[9,    30] loss: 0.001\n",
      "[9,    40] loss: 0.001\n",
      "[9,    50] loss: 0.001\n",
      "[9,    60] loss: 0.001\n",
      "[10,    10] loss: 0.001\n",
      "[10,    20] loss: 0.001\n",
      "[10,    30] loss: 0.001\n",
      "[10,    40] loss: 0.001\n",
      "[10,    50] loss: 0.001\n",
      "[10,    60] loss: 0.001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0794, Accuracy: 1466/1500 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in testloader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = net(data)\n",
    "    test_loss += F.cross_entropy(output, target, size_average=False).data[0] # sum up batch loss\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(testloader.dataset),\n",
    "    100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 200)\n",
    "        #self.do = nn.Dropout()\n",
    "        #self.fc2 = nn.Linear(300, 200)\n",
    "        #self.fc3 = nn.Linear(500, 100)\n",
    "        self.fc4 = nn.Linear(200, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.do(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.1062, Accuracy: 2885/3000 (96%)\n",
      "Test set: Average loss: 0.1103, Accuracy: 2881/3000 (96%)\n",
      "Test set: Average loss: 0.1045, Accuracy: 2884/3000 (96%)\n",
      "Test set: Average loss: 0.1046, Accuracy: 2881/3000 (96%)\n",
      "Test set: Average loss: 0.1049, Accuracy: 2887/3000 (96%)\n",
      "Test set: Average loss: 0.1117, Accuracy: 2884/3000 (96%)\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=6, test_size=0.2)\n",
    "for train, test in sss.split(X, y):\n",
    "    X_train = X[train,:]\n",
    "    y_train = y[train]\n",
    "    X_test = X[test,:]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    trainset = torch.utils.data.TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train))\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=300, shuffle=True, num_workers=2)\n",
    "    testset = torch.utils.data.TensorDataset(torch.Tensor(X_test), torch.LongTensor(y_test))\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=300, shuffle=True, num_workers=2)\n",
    "    \n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.002, weight_decay=2e-4)\n",
    "    \n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            #running_loss = loss.data[0]\n",
    "            #print('[%d] loss: %.3f' % (epoch + 1, running_loss))\n",
    "\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in testloader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = net(data)\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "            test_loss, correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
